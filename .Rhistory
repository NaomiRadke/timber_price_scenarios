install.packages("ggplot2")
library(ggplot2)
install.packages("ggplot2")
.libPaths()
install.packages("ggplot2")
install.packages("knitr")
traceback()
install.packages("shiny")
library("shiny")
install.packages("TapeR")
library(TaperR)
library(TapeR)
?TapeR
rm(list=ls())
k = 0.694614
p = 0.0862735
q = 0.135984
r = dbh/2
dbh = c(7:30)# diameter of diameters at breast height
i = k*dbh
H = c(11.63590383,
13.08553379,
14.29547995,
15.31596808,
16.18589136,
16.93498741,
17.58604689,
18.15668046,
18.66063857,
19.10877637,
19.50975687,
19.8705658,
20.19689173,
20.49340934,
20.76399298,
21.01187904,
21.23979089,
21.45003584,
21.64458113,
21.8251139,
21.9930891,
22.14976788,
22.29624865,
22.43349244) # vector with total tree heights of a certain dbh
h = H/2# height at which we want the diameter
u = (i/(1-exp(q*(1.3-H)))) + ((dbh/2)-i)*(1-(1/(1-exp(p*(1.3-H)))))
v = (((dbh/2)-i)*exp(p*1.3))/(1-exp(p*(1.3-H)))
w = (i*exp(-q*H))/(1-exp(q*(1.3-H)))
dbh_mid = 2*(u+v*exp(-p*h)-w*exp(q*h))
dbh_mid
install.packages("dplyr")
RMSE_param_best <- RMSE_param_new %>% filter(RMSE < 12)
installed.packages("knitr")
install.packages("knitr")
updateR()
install.packages("knitr")
update.packages(ask = FALSE, repos = 'https://cran.rstudio.org')
update.packages(ask = FALSE, repos = 'https://cran.rstudio.org')
install.packages("knitr")
install.packages('knitr', repos = c('https://xran.yihui.name', 'https://cran.rstudio.org'))
RMSE_param_best <- RMSE_param_new %>% filter(RMSE < 12)
install.packages("tidyverse")
installed.packages("knitr")
library(knitr)
RMSE_param_best <- RMSE_param_new %>% filter(RMSE < 12)
install.packages("dplyr")
installed.packages("dplyr")
library("dplyr")
install.packages("dplyr")
.libPaths()
install.packages("knitr")
library(knitr)
install.packages("dplyr")
library(dplyr)
.rs.restartR()
rm(list=ls())
dev.off()
installed.packages(dplyr)
install.packages("dplyr")
library(dplyr)
install.packages("tidyverse")
library(tidyverse)
install.packages("broom")
library(tidyverse)
install.packages("tidyverse", dependencies = TRUE)
library(tidyverse)
library(tidyverse)
update.packages()
y
library(dplyr)
if(!require(installr)) {
install.packages("installr");
require(installr)
}
install.packages("installr")
require(installr)
install.packages("stringr")
require("installr")
library("stringr")
library(installr)
library(stringr)
.libPaths()
?install.packages()
install.packages("installr", C:\Program Files\R\R-3.3.3\library)
library(installr)
library(installr)
library(installr)
library(tidyverse)
packages <- installed.packages()
packages
write.csv(packages, "mypackages.csv" )
getwd()
log(0)
log(-1)
?floor
a <- c(NA, NA, NA)
sum(a)
0/3
library(compiler)
?enableJIT
setwd("~/")
rm(list = ls())
graphics.off()
# Install packages if necessary, and load them.
# install.packages("lhs")
# install.packages("compiler")
library(lhs)
library(compiler)
enableJIT(3)
dir.create("lab_X")
dir.path <- paste(getwd(), "/lab_X/", sep="")
scrim.git.path <- "https://raw.githubusercontent.com/scrim-network/"
scrim.git.path <- "https://raw.githubusercontent.com/scrim-network/"
# Sea-level change data
url <- paste(scrim.git.path, "BRICK/master/data/GMSL_ChurchWhite2011_yr_2015.txt", sep="")
church.data <- read.table("lab_X/GMSL_ChurchWhite2011_yr_2015.txt")
download.file(url, paste(dir.path, "GMSL_ChurchWhite2011_yr_2015.txt", sep=""))
church.data <- read.table("lab_X/GMSL_ChurchWhite2011_yr_2015.txt")
year.hist <- floor(church.data[ ,1])
slr.hist <- church.data[ ,2]
slr.err <- church.data[ ,3]
?floor()
View(church.data)
url <- paste(scrim.git.path,
"Ruckertetal_SLR2016/master/RFILES/Data/NOAA_IPCC_RCPtempsscenarios.csv",
sep="")
download.file(url, paste(dir.path, "NOAA_IPCC_RCPtempsscenarios.csv", sep=""))
temp.data <- read.csv("lab_X/NOAA_IPCC_RCPtempsscenarios.csv")
temp.hist <- temp.data$Historical.NOAA.temp.
?match()
ibeg <- match(year.hist[1], temp.data$Time)
iend <- match(max(year.hist), temp.data$Time)
a <- year.hist[1] %in% temp.data$Time
temp.hist <- temp.hist[ibeg:iend]
ind.norm <- match(1951, year.hist):match(1980, year.hist)
temp.hist <- temp.hist - mean(temp.hist[ind.norm])
temp.hist[ind.norm]
temp.hist
temp.hist <- temp.data$Historical.NOAA.temp.
View(temp.data)
temp.data$Historical.NOAA.temp..respect.to.20th.century
temp.hist <- temp.hist[ibeg:iend]
temp.hist <- temp.data$Historical.NOAA.temp..respect.to.20th.century
temp.hist[ibeg:iend]
temp.hist <- temp.hist[ibeg:iend]
ind.norm <- match(1951, year.hist):match(1980, year.hist)
temp.hist <- temp.hist - mean(temp.hist[ind.norm])
slr.hist <- slr.hist - mean(slr.hist[ind.norm])
ibeg <- match(year.hist[1], temp.data$Time)
iend <- match(2100, temp.data$Time)
temp.proj <- temp.data$Historical.NOAA.temp...RCP.8.5[ibeg:iend]
year.proj <- temp.data$Time[1:match(2100, temp.data$time)]
View(temp.data)
year.proj <- temp.data$Time[1:match(2100, temp.data$Time)]
ind.norm <- match(1951, year.proj):match(1980, year.proj)
temp.proj <- temp.proj - mean(temp.proj[ind.norm])
View(temp.data)
temp.proj <- temp.data$Historical.NOAA.temp...CNRM.RCP.8.5.with.respect.to.20th.century[ibeg:iend]
year.proj <- temp.data$Time[1:match(2100, temp.data$Time)]
ind.norm <- match(1951, year.proj):match(1980, year.proj)
temp.proj <- temp.proj - mean(temp.proj[ind.norm])
url <- paste(scrim.git.path, "BRICK/master/R/gmsl_r07.R", sep="")
download.file(url, paste(dir.path, "gmsl_r07.R", sep=""))
source("lab_X/gmsl_r07.R")
tstep = 1
model0 <- gmsl_r07(sl_temp_sens=3.4, temp_equil=-0.5, sl0=slr.hist[1],
tstep=tstep, temperature_forcing=temp.hist)
url
model0 <- gmsl_r07(a=3.4, Teq=-0.5, sl0=slr.hist[1],
tstep=tstep, Tg=temp.hist)
model0 <- gmsl_r07(a=3.4, Teq=-0.5, sl0=slr.hist[1],
tstep=tstep, Tg=temp.hist)
source("lab_X/gmsl_r07.R")
tstep = 1
model0 <- gmsl_r07(a=3.4, Teq=-0.5, sl0=slr.hist[1],
tstep=tstep, Tg=temp.hist)
source("lab_X/gmsl_r07.R")
tstep = 1
model0 <- gmsl_r07(a=3.4, Teq=-0.5, sl0=slr.hist[1],
tstep=tstep, Tg=temp.hist)
slr.hist[1]
model0 <- gmsl_r07(a=3.4, Teq=-0.5, sl0=slr.hist[1],
tstep=tstep, Tg=temp.hist)
model0 <- gmsl_r07(a=3.4, Teq=-0.5, sl0=-118,
tstep=tstep, Tg=temp.hist)
model0 <- gmsl_r07(a=3.4, Teq=-0.5, Sl0=slr.hist[1],
tstep=tstep, Tg=temp.hist)
model0 <- gmsl_r07(a=3.4, Teq=-0.5, Sl0=slr.hist[1],
tstep=tstep, Tg=temp.hist)
source("lab_X/gmsl_r07.R")
model0 <- gmsl_r07(a=3.4, Teq=-0.5, Sl0=slr.hist[1],
tstep=tstep, Tg=temp.hist)
plot(year.hist, slr.hist, type='p', col='red', xlab='Year', ylab='Sea level [mm]')
lines(year.hist, model0, lwd=2)
legend(1880, 200, c('Model 0','Data'), lty=c(1,NA),
pch=c(NA,1), col=c('black','red'), bty='n')
model1 <- gmsl_r07(a=3.74, Teq=-0.5, sl0=slr.hist[1],
tstep=tstep, Tg=temp.hist)
model1 <- gmsl_r07(a=3.74, Teq=-0.5, Sl0=slr.hist[1],
tstep=tstep, Tg=temp.hist)
plot(year.hist, slr.hist, type='p', col='red', xlab='Year', ylab='Sea level [mm]')
lines(year.hist, model1, lwd=2)
legend(1880, 200, c('Model 1','Data'), lty=c(1,NA),
pch=c(NA,1), col=c('black','red'), bty='n')
sens_2013 <- function(parameters, temperature_forcing, tstep=1) {
# Run the GMSL model with the given parameters and forcing
model <- gmsl_r07(a=parameters[1],
Teq=parameters[2],
Sl0=parameters[3],
tstep=tstep,
Tg=temperature_forcing)
# Grab and return the sea level from 2013, the last year
return(model[length(model)])
}
set.seed(11)
n = 10
alphas <- runif(min=0, max=5, n=n)
Teqs <- runif(min=-1.5, max=0.5, n=n)
plot(alphas, Teqs, xlim=c(0,5), ylim=c(-1.5,0.5),
xlab=expression(alpha), ylab=expression(T[eq]))
set.seed(11)
# Draw a Latin hypercube sample with k=2 parameters and n samples
parameters <- randomLHS(k=2, n=n)
# Scale the parameters
alphas <- parameters[,1]*5
Teqs <- parameters[,2]*2 - 1.5
# Make a scatter plot of the parameter samples
plot(alphas, Teqs, xlim=c(0, 5), ylim=c(-1.5, 0.5),
xlab=expression(alpha), ylab=expression(T[eq]))
sens_2013 <- function(parameters, temperature_forcing, tstep=1) {
# Run the GMSL model with the given parameters and forcing
model <- gmsl_r07(a=parameters[1],
Teq=parameters[2],
Sl0=parameters[3],
tstep=tstep,
Tg=temperature_forcing)
# Grab and return the sea level from 2013, the last year
return(model[length(model)])
}
set.seed(11)
# draw parameters
n <- 1000
parameters <- randomLHS(k=2, n=n)
# scale the parameters appropriately
alphas <- parameters[,1] <- parameters[,1]*5
Teqs <- parameters[,2] <- parameters[,2]*2 - 1.5
# create an array to store all of the necessary parameters, including S0
parameters <- cbind(parameters, rep(slr.hist[1], n))
# run the model with Latin hypercube parameters, and calculate sensitivity measure
sens_lhs <- sapply(1:n, function(s) sens_2013(parameters[s,], temp.hist, tstep))
# interpolate the irregular results to a grid
fld <- interp(alphas,Teqs,sens_lhs)
# make a contour plot
contour(fld, xlab=expression(alpha), ylab=expression(T[eq]))
install.packages("interp")
library(interp)
set.seed(11)
# draw parameters
n <- 1000
parameters <- randomLHS(k=2, n=n)
# scale the parameters appropriately
alphas <- parameters[,1] <- parameters[,1]*5
Teqs <- parameters[,2] <- parameters[,2]*2 - 1.5
# create an array to store all of the necessary parameters, including S0
parameters <- cbind(parameters, rep(slr.hist[1], n))
# run the model with Latin hypercube parameters, and calculate sensitivity measure
sens_lhs <- sapply(1:n, function(s) sens_2013(parameters[s,], temp.hist, tstep))
# interpolate the irregular results to a grid
fld <- interp(alphas,Teqs,sens_lhs)
# make a contour plot
contour(fld, xlab=expression(alpha), ylab=expression(T[eq]))
rm(list=ls())
?dataframe.in
?map.range()
install.packages(DataCombine)
install.packages("DataCombine")
library(dplyr)
library(zoo)
library(DataCombine)
df = expand.grid(site = factor(seq(10)),
year = 2000:2004,
day = 1:50)
View(df)
df$temp = rpois(dim(df)[1], 5)
# Assume rains 33% of the days and averages 5 mm each time but highly variable
df$precip = rbinom(dim(df)[1], 1, 1/3) * rlnorm(dim(df)[1], log(5), 1)
View(df)
df = df[order(df$site, df$year, df$day), ]
df.slide = slide(df, Var = "temp", GroupVar = c("site", "year"), slideBy = -1, NewVar='temp.lag1')
head(df.slide, 75)
rm(list=ls())
df = expand.grid(site = factor(seq(10)),
year = 2000:2004,
day = 1:50)
# use Poisson to make math easy to check moving means of temperature
df$temp = rpois(dim(df)[1], 5)
# Assume rains 33% of the days and averages 5 mm each time but highly variable
df$precip = rbinom(dim(df)[1], 1, 1/3) * rlnorm(dim(df)[1], log(5), 1)
df2 = df %>%
group_by(site, year) %>%
arrange(site, year, day) %>%
mutate(temp.5 = rollmean(x = temp, 5, align = "right", fill = NA))
View(df2)
View(df2)
View(df)
df3 = df2 %>%
mutate(temp.lag1 = lag(temp, n = 1)) %>%
mutate(temp.5.previous = rollapply(data = temp.lag1,
width = 5,
FUN = mean,
align = "right",
fill = NA,
na.rm = T))
View(df3)
load("~/Step 2_applying OpenMORDM/Case study/Precalibration/model_precalibration/for_Cluster/DEoptim_out2018-11-05.RData")
plot(out_DEoptim)
load("~/Step 2_applying OpenMORDM/Case study/Precalibration/model_precalibration/for_Cluster/DEoptim_out2018-11-07.RData")
library(DEoptim)
?Deoptim()
?DEoptim()
plot(out_DEoptim, plot.type = "bestmemit")
plot(out_DEoptim, plot.type = "bestmemit")
library(DEoptim)
?DEoptim(DEoptim)
ls(globalenv())
?sprintf
scen_model_params <- sprintf("p_%d", seq(1:20))
scen_DI <- sprintf("DI_%d", seq(1:20))
scen_DR <- sprintf("DR_%d", seq(1:5))
scen_DR <- sprintf("DR_%d", seq(1:5))
scen_WP <- sprinf("WP_%d", seq(1:20))
scen_CT <- sprintf("CT_%d", seq(1:20))
scen_WP <- sprintf("WP_%d", seq(1:20))
scenarios <- expand.grid(scen_model_params, scen_DI, scen_DR, scen_WP, scen_CT)
View(scenarios)
scenarios <- expand.grid("model" = scen_model_params, "DI" = scen_DI, "DR" = scen_DR, "WP" = scen_WP, "CT" = scen_CT)
View(scenarios)
View(scenarios)
scen_model_params <- sprintf("p_%d", seq(1:20))
scen_DI <- seq(1:20)
scen_DR <- seq(1:5)
scen_WP <- seq(1:20)
scen_CT <- seq(1:20)
# Create a total enumeration of individual scenarios to create total scenarios
scenarios <- expand.grid("model" = scen_model_params, "DI" = scen_DI, "DR" = scen_DR, "WP" = scen_WP, "CT" = scen_CT)
View(scenarios)
scen_model_params <- seq(1:20)
scenarios <- expand.grid("model" = scen_model_params, "DI" = scen_DI, "DR" = scen_DR, "WP" = scen_WP, "CT" = scen_CT)
View(scenarios)
load("~/Step 2_applying OpenMORDM/Case study/Precalibration/model_precalibration/for_Cluster/DEoptim_out_vol_2018-11-15.RData")
library(DEoptim)
plot(out_DEoptim, plot.type = "bestmemit")
plot(out_DEoptim, plot.type = "bestmemit")
rm(list= ls())
setwd("~/Step2_MORDM/Case_study/timber_price_scenarios")
library(forecast) # for fitting arima and forecasting
library(tseries)  # for turning data into time series data
library(ggplot2)  # for plotting
# Load price data as time series
WP <- read.csv("Data/Preis_Index_Buch_1971_2016.csv", sep = ";")# read the data from csv
WP_ts <- ts(WP$price_index, start = 1971)                  # turn it into time series
plot.ts(WP_ts)                                             # check for stationarity
#### MANUAL AND AUTOMATED ARIMA ######################################
WP_ts_diff1 <- diff(WP_ts, differences = 1)
plot(WP_ts_diff1)
# Conduct formal test for stationarity
# e.g. with the Augmented Dickey-Fuller unit root test (p-val should be below 0.05/5%)
print(adf.test(WP_ts_diff1))
# STEP 2: Identifying p and q
# making a correlogram and partial correlogram
acf(WP_ts_diff1)    # shows that q is 1
pacf(WP_ts_diff1)   # shows that p is 0
WP_ts_arima <- Arima(WP_ts, order = c(1,1,0), include.mean = FALSE) # This could be our arima model
# Let's see what the automated arima function gives us
auto <- auto.arima(WP_ts, stepwise = FALSE, approximation = FALSE, seasonal = FALSE)  # let the auto.arima function do that for us
# stepwise+approximation set to FALSE to
# consider all possible models in the search
summary(auto) # shows the fitted arima model
# STEP 3: Check the residuals
checkresiduals(auto)  # check for normality / ACF = whether autorcorrelations are within threshold
# STEP 4: Forecast
# for the manual arima
WP_forecast <- forecast(WP_ts_arima, h = 60)
summary(WP_forecast)
# for the automated arima
for_auto_arima <- forecast(auto, h = 60)
# plot the best fit arima -> will add uncertain forecast scenario lines to the graph lateron
g <- autoplot(for_auto_arima)
####### CREATING UNCERTAIN SCENARIOS #########################
# STEP 1: Create a vector of residuals (fitted vs. data)
resids <- auto$residuals
# STEP 2: Make n.boot bootstraps of the residuals
n.boot = 100 # define number of samples
t = 60      # number of years for forecast
set.seed(2019)
boot.resids <- data.frame(replicate(n.boot, sample(resids, length(resids), replace = TRUE)))
boot.for <- data.frame(matrix(0, ncol = t, nrow = n.boot)) # initialize a vector for the forecasts
# STEP 3: For each bootstrap add residuals to best fit arima model, fit a new model and make a forecast
for(i in 1:n.boot){
# add the residuals to the best fit arima model
boot.WP <- auto$fitted + boot.resids[,i]
# fit an arima model to the new data
auto_boot <- auto.arima(boot.WP, stepwise = FALSE, approximation = FALSE, seasonal = FALSE)
# forecast new model
for_auto_boot <- forecast(auto_boot, h=t)
# save forecasted WP scenarios in a data frame
boot.for[i,] <- for_auto_boot$mean
g <- g + autolayer(for_auto_boot$mean)
}
g+
scale_color_manual(labels = c("bootstrap"), values = c(2))+
ggtitle("Forecasts from ARIMA(1,0,0) and resampled residuals")
?arima.sim
?bld.mbb.bootstrap()
?simulate.Arima
?arima.sim()
end(WP_ts)
end(WP_ts)[1]
n.yrs = 30 # number of forecasted years
n.sim = 100 # number of sampled future trajectories
sim <- ts(matrix(0, nrow = n.yrs, ncol = n.sim), start = end(WP_ts)[1]+1)
for(i in seq(n.sim))
for(i in 1:n.sim){
sim[,i] <- simulate(auto, nsim = n.yrs)
}
autoplot(WP_ts)+
autolayer(sim)
autoplot(WP_ts[2])+
autolayer(sim)
autoplot(WP_ts[2])+ autolayer(sim)
autoplot(WP_ts[2])+ autolayer(sim)
autoplot(WP_ts[2])+
autolayer(sim)+
guides(legend = "none")
autoplot(auto)+
autolayer(sim)+
guides(legend = "none")
plot.ts(WP_ts)                                             # check for stationarity
dev.off()
plot.ts(WP_ts)                                             # check for stationarity
autoplot(WP_ts)
+autolayer(sim)
n.sim = 10 # number of sampled future trajectories
sim <- ts(matrix(0, nrow = n.yrs, ncol = n.sim), start = end(WP_ts)[1]+1) # empty ts matrix to be filled with simulations
for(i in 1:n.sim){
sim[,i] <- simulate(auto, nsim = n.yrs)
}
autoplot(auto)+
autolayer(sim)+
guides(legend = "none")
autoplot(WP_ts)+
autolayer(sim)+
guides(legend = "none")
autoplot(WP_ts)+
autolayer(sim)+
guides(legend = "none")
autoplot(WP_ts)+
autolayer(sim)+
labs(title = "Wood price development and plausible future paths", y = "Wood price index")+
theme(legend.position = "none")
autoplot(WP_ts)+
autolayer(sim)+
labs(title = "Wood price development and plausible future paths", y = "Wood price index")+
theme(legend.position = "none")+
ylim(c(0,250))
autoplot(WP_ts)+
autolayer(sim)+
labs(title = "Wood price development and plausible future paths", y = "Wood price index")+
theme(legend.position = "none")+
theme_bw()
autoplot(WP_ts)+
ylim(c(0,250))+
autolayer(sim)+
labs(title = "Wood price development and plausible future paths", y = "Wood price index")+
theme_bw()+
theme(legend.position = "none")
